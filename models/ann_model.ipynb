{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0cfd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "35ee27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"                Model Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"R-squared (R2 ): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "68086022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(input, y_pred, key='Validation', filename='../output/ann_model_predictions.csv'):\n",
    "    output = input[['DATETIME', 'ENTITY_DESCRIPTION_SHORT']].copy()\n",
    "    output['y_pred'] = y_pred\n",
    "    output['KEY'] = [key] * len(y_pred)\n",
    "    output.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b1804af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_route = '../data/waiting_times_train.csv'\n",
    "validation_route = '../data/waiting_times_X_test_val.csv'\n",
    "test_route = '../data/waiting_times_X_test_final.csv'\n",
    "weather_route = '../data/weather_data.csv'\n",
    "\n",
    "train_file = pd.read_csv(train_route)\n",
    "validation_file = pd.read_csv(validation_route)\n",
    "test_file = pd.read_csv(test_route)\n",
    "weather_file = pd.read_csv(weather_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97ad5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_file.copy()\n",
    "X_train['DATETIME'] = pd.to_datetime(X_train['DATETIME'], errors='coerce')\n",
    "\n",
    "weather_file['DATETIME'] = pd.to_datetime(weather_file['DATETIME'], errors='coerce')\n",
    "weather_file = weather_file.fillna(0)\n",
    "\n",
    "X_train = pd.merge(X_train, weather_file, on='DATETIME', how='left')\n",
    "X_train = pd.get_dummies(X_train, columns=['ENTITY_DESCRIPTION_SHORT'], drop_first=True, dtype=int)\n",
    "X_train['year'] = X_train['DATETIME'].dt.year\n",
    "X_train['month'] = X_train['DATETIME'].dt.month\n",
    "X_train['day'] = X_train['DATETIME'].dt.day\n",
    "X_train['hour'] = X_train['DATETIME'].dt.hour\n",
    "X_train['minute'] = X_train['DATETIME'].dt.minute\n",
    "\n",
    "X_train['TIME_TO_PARADE_1'] = X_train['TIME_TO_PARADE_1'].fillna(250)\n",
    "X_train['TIME_TO_PARADE_2'] = X_train['TIME_TO_PARADE_2'].fillna(0)\n",
    "\n",
    "X_train = X_train.sort_values(by='DATETIME')\n",
    "\n",
    "# Fill NaNs by column\n",
    "for col in X_train.select_dtypes(include=['number']).columns:\n",
    "    # Compute mean and std of non-NaN values\n",
    "    mean = X_train[col].mean()\n",
    "    std = X_train[col].std()\n",
    "    \n",
    "    # Find number of NaNs\n",
    "    n_nan = X_train[col].isna().sum()\n",
    "    \n",
    "    # Generate random numbers for NaNs\n",
    "    random_values = np.random.normal(loc=mean, scale=std, size=n_nan)\n",
    "    \n",
    "    # Fill NaNs\n",
    "    X_train.loc[X_train[col].isna(), col] = random_values\n",
    "\n",
    "\n",
    "y_train = X_train['WAIT_TIME_IN_2H']\n",
    "X_train = X_train.select_dtypes(include=['number']).drop(columns=['clouds_all', 'humidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a2cfbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristicas_seleccionadas = X_train.columns.tolist()\n",
    "X_train_aux = X_train.copy().drop(columns=['WAIT_TIME_IN_2H'])\n",
    "scaler = StandardScaler()\n",
    "df_escalado = scaler.fit_transform(X_train_aux)\n",
    "X_train = X_train.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b6197e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataset_multivariante(dataset, lookback=1):\n",
    "    X = []\n",
    "    for i in range(lookback, len(dataset)):\n",
    "        X.append(dataset[i-lookback:i, :]) # Tomamos TODAS las columnas para la ventana\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "63797352",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4476cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36988, 30, 20)\n"
     ]
    }
   ],
   "source": [
    "target_col_index = caracteristicas_seleccionadas.index('WAIT_TIME_IN_2H')\n",
    "X = crear_dataset_multivariante(df_escalado, lookback)\n",
    "print(X.shape)\n",
    "y = y_train[lookback:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f4520e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fd1e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[2] # Número de características (columnas)\n",
    "\n",
    "input_layer = Input(shape=(lookback, n_features))\n",
    "lstm1 = LSTM(50, return_sequences=True)(input_layer)\n",
    "drop1 = Dropout(0.2)(lstm1)\n",
    "lstm2 = LSTM(50, return_sequences=False)(drop1)\n",
    "drop2 = Dropout(0.2)(lstm2)\n",
    "output_layer = Dense(1)(drop2)\n",
    "\n",
    "modelo_lstm_multi = Model(inputs=input_layer, outputs=output_layer)\n",
    "modelo_lstm_multi.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6582caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 223.3268 - val_loss: 576.9011\n",
      "Epoch 2/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - loss: 130.9437 - val_loss: 635.0412\n",
      "Epoch 3/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - loss: 118.1522 - val_loss: 341.7314\n",
      "Epoch 4/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 111.6893 - val_loss: 366.4337\n",
      "Epoch 5/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 107.5115 - val_loss: 314.1693\n",
      "Epoch 6/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 105.1733 - val_loss: 298.8990\n",
      "Epoch 7/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 101.8243 - val_loss: 267.6361\n",
      "Epoch 8/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 98.5963 - val_loss: 271.1489\n",
      "Epoch 9/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 96.1596 - val_loss: 237.5972\n",
      "Epoch 10/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - loss: 95.2839 - val_loss: 255.4131\n",
      "Epoch 11/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - loss: 93.4502 - val_loss: 235.8275\n",
      "Epoch 12/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 91.7900 - val_loss: 254.4453\n",
      "Epoch 13/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 90.5761 - val_loss: 269.5403\n",
      "Epoch 14/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 88.9320 - val_loss: 263.8694\n",
      "Epoch 15/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 88.4259 - val_loss: 272.0266\n",
      "Epoch 16/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - loss: 86.5462 - val_loss: 276.5439\n",
      "Epoch 17/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - loss: 84.9271 - val_loss: 269.7461\n",
      "Epoch 18/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 84.7340 - val_loss: 266.0605\n",
      "Epoch 19/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 83.8690 - val_loss: 284.1146\n",
      "Epoch 20/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - loss: 83.0409 - val_loss: 279.3340\n",
      "Epoch 21/100\n",
      "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 81.7001 - val_loss: 291.2833\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = modelo_lstm_multi.fit(X_train, y_train,\n",
    "                                epochs=100,\n",
    "                                batch_size=32,\n",
    "                                validation_data=(X_test, y_test),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6a863444",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validation_file.copy()\n",
    "X_val['DATETIME'] = pd.to_datetime(X_val['DATETIME'], errors='coerce')\n",
    "X_val_aux = X_val.copy()\n",
    "\n",
    "X_val = pd.merge(X_val, weather_file, on='DATETIME', how='left')\n",
    "X_val = pd.get_dummies(X_val, columns=['ENTITY_DESCRIPTION_SHORT'], drop_first=True, dtype=int)\n",
    "X_val['year'] = X_val['DATETIME'].dt.year\n",
    "X_val['month'] = X_val['DATETIME'].dt.month\n",
    "X_val['day'] = X_val['DATETIME'].dt.day\n",
    "X_val['hour'] = X_val['DATETIME'].dt.hour\n",
    "X_val['minute'] = X_val['DATETIME'].dt.minute\n",
    "\n",
    "X_val['TIME_TO_PARADE_1'] = X_val['TIME_TO_PARADE_1'].fillna(250)\n",
    "X_val['TIME_TO_PARADE_2'] = X_val['TIME_TO_PARADE_2'].fillna(0)\n",
    "\n",
    "X_val = X_val.sort_values(by='DATETIME')\n",
    "\n",
    "# Fill NaNs by column\n",
    "for col in X_val.select_dtypes(include=['number']).columns:\n",
    "    # Compute mean and std of non-NaN values\n",
    "    mean = X_val[col].mean()\n",
    "    std = X_val[col].std()\n",
    "    \n",
    "    # Find number of NaNs\n",
    "    n_nan = X_val[col].isna().sum()\n",
    "    \n",
    "    # Generate random numbers for NaNs\n",
    "    random_values = np.random.normal(loc=mean, scale=std, size=n_nan)\n",
    "    \n",
    "    # Fill NaNs\n",
    "    X_val.loc[X_val[col].isna(), col] = random_values\n",
    "\n",
    "\n",
    "X_val = X_val.select_dtypes(include=['number']).drop(columns=['clouds_all', 'humidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ee4f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DATETIME ENTITY_DESCRIPTION_SHORT  ADJUST_CAPACITY  DOWNTIME  \\\n",
      "0 2019-11-23 10:45:00               Water Ride            247.0         0   \n",
      "1 2022-01-03 16:45:00              Pirate Ship            153.0         0   \n",
      "2 2021-12-04 15:30:00              Pirate Ship            255.0         0   \n",
      "3 2020-02-05 13:15:00               Water Ride            247.0         0   \n",
      "4 2022-05-13 15:15:00           Flying Coaster            756.0         0   \n",
      "\n",
      "   CURRENT_WAIT_TIME  TIME_TO_PARADE_1  TIME_TO_PARADE_2  TIME_TO_NIGHT_SHOW  \n",
      "0                 20             375.0              75.0               675.0  \n",
      "1                 45               NaN               NaN                 NaN  \n",
      "2                 40               NaN               NaN                 NaN  \n",
      "3                 15             225.0               NaN               345.0  \n",
      "4                 35             135.0               NaN               465.0  \n"
     ]
    }
   ],
   "source": [
    "print(X_val_aux.head())\n",
    "df_escalado = scaler.transform(X_val)\n",
    "X = crear_dataset_multivariante(df_escalado, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "967bf477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "X = crear_dataset_multivariante(df_escalado, lookback)\n",
    "y_pred = modelo_lstm_multi.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "40e6dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATETIME', 'ENTITY_DESCRIPTION_SHORT', 'ADJUST_CAPACITY', 'DOWNTIME',\n",
      "       'CURRENT_WAIT_TIME', 'TIME_TO_PARADE_1', 'TIME_TO_PARADE_2',\n",
      "       'TIME_TO_NIGHT_SHOW', 'y'],\n",
      "      dtype='object')\n",
      "Predictions saved to ../output/ann_model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "X_val_aux['y'] = [number[0] for number in y_pred.tolist()] + [np.mean(y_train)]*lookback\n",
    "X_val_aux = X_val_aux.sort_index()\n",
    "print(X_val_aux.columns)\n",
    "generate_csv(X_val_aux, X_val_aux['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
